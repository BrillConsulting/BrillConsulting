# ü§ñ Machine Learning Portfolio

Professional Machine Learning projects showcasing classic ML algorithms with production-ready implementations, automatic model selection, and comprehensive evaluation.

## üÜï Latest Updates (v2.0)

**Enhanced Projects with Production-Ready Implementations:**
- **FeatureSelection** ‚≠ê - Complete rewrite with 7+ selection methods (Univariate, MI, RFE, Tree Importance, Lasso, Permutation, Ensemble Voting) - 665 lines
- **ImbalancedLearning** ‚≠ê - Advanced techniques with SMOTE, ADASYN, Under-sampling, Tomek Links, and hybrid methods - 579 lines
- **AutoML** ‚≠ê - Automated ML with hyperparameter optimization, model selection (6+ algorithms), and intelligent task detection - 501 lines
- **RegressionAnalysis** ‚≠ê - 10+ algorithms including XGBoost, LightGBM, Random Forest, SVR, Gradient Boosting with hyperparameter tuning - 600+ lines
- **ClassificationModels** ‚≠ê - 14+ algorithms including XGBoost, LightGBM, Voting/Stacking ensembles, ROC curves, feature importance - 498 lines
- **TimeSeriesForecasting** ‚≠ê **NEW** - 10+ forecasting models (ARIMA, SARIMA, Auto-ARIMA, Prophet, Exp Smoothing) with stationarity testing, decomposition, anomaly detection - 709 lines
- **EnsembleMethods** ‚≠ê **NEW** - 10+ ensemble methods (Bagging, Random Forest, Extra Trees, AdaBoost, GB, XGBoost, LightGBM, Voting, Stacking) for classification/regression - 782 lines

**Total Enhancement:** 7 projects upgraded with ~4,334+ lines of production-ready code. All include comprehensive implementations, advanced diagnostics, hyperparameter tuning, and detailed documentation.

## üì¶ Projects Overview (15 Projects)

### 1. üìà [Regression Analysis](RegressionAnalysis/) ‚≠ê **UPGRADED v2.0**
Production-ready regression with 10+ algorithms including advanced gradient boosting.

**Algorithms:**
- **Linear Models**: Linear, Ridge, Lasso, Polynomial
- **Tree-Based**: Random Forest, Gradient Boosting
- **Advanced**: XGBoost, LightGBM (optional)
- **Other**: Support Vector Regression (SVR)

**Key Features:**
- 10+ algorithms with GridSearchCV hyperparameter tuning
- Residual analysis with normality tests
- Feature importance for tree-based models
- Advanced diagnostics and model comparison
- Automatic best model selection

**Technologies:** scikit-learn, pandas, matplotlib, XGBoost*, LightGBM*

**Status:** 600+ lines, production-ready with comprehensive evaluation

```bash
cd RegressionAnalysis
python regression_models.py --data housing.csv --target price --output results.png
```

---

### 2. üéØ [Classification Models](ClassificationModels/) ‚≠ê **UPGRADED v2.0**
Advanced classification with 14+ algorithms and ensemble methods.

**Algorithms:**
- **Traditional ML**: Logistic Regression, SVM, KNN, Naive Bayes
- **Tree-Based**: Decision Tree, Random Forest, Extra Trees
- **Boosting**: Gradient Boosting, AdaBoost, XGBoost*, LightGBM*
- **Ensemble**: Voting Classifier, Stacking Classifier

**Key Features:**
- 14+ algorithms with optional GridSearchCV tuning
- ROC curves for binary classification
- Confusion matrices for all models
- Feature importance visualization
- Voting and Stacking ensemble methods
- Comprehensive metrics (Accuracy, Precision, Recall, F1, ROC-AUC)

**Technologies:** scikit-learn, seaborn, XGBoost*, LightGBM*

**Status:** 498 lines, production-ready with ensemble methods

```bash
cd ClassificationModels
python classifiers.py --data data.csv --target label --output-cm confusion.png --output-roc roc.png
```

---

### 3. üé® [Clustering](Clustering/)
Unsupervised clustering with automatic cluster number selection.

**Algorithms:**
- K-Means (with elbow method)
- DBSCAN (density-based)
- Hierarchical Clustering

**Key Features:**
- Silhouette score optimization
- PCA visualization
- Elbow method for K-Means
- Dendrogram generation

**Technologies:** scikit-learn, scipy

```bash
cd Clustering
python clustering_analysis.py --data customers.csv --output clusters.png
```

---

### 4. üìà [Time Series Forecasting](TimeSeriesForecasting/) ‚≠ê **UPGRADED v2.0**
Production-ready time series forecasting with 10+ models and comprehensive diagnostics.

**Models:**
- **Baseline**: Naive, Seasonal Naive, Moving Average
- **Statistical**: ARIMA, SARIMA, Auto-ARIMA*, Exponential Smoothing
- **Advanced**: Prophet* (Facebook)

**Key Features:**
- 10+ forecasting models with automatic selection
- Stationarity testing (ADF test)
- ACF/PACF analysis for parameter selection
- Time series decomposition (trend, seasonal, residual)
- Anomaly detection (IQR, Z-score)
- Comprehensive metrics (RMSE, MAE, MAPE, SMAPE)
- Residual diagnostics (Ljung-Box test)
- Walk-forward validation

**Technologies:** statsmodels, scikit-learn, prophet*, pmdarima*

**Status:** 709 lines, production-ready with full diagnostics

```bash
cd TimeSeriesForecasting
python time_series.py --data sales.csv --steps 12 --seasonal-period 12 --output forecast.png
```

---

### 5. üé≠ [Ensemble Methods](EnsembleMethods/) ‚≠ê **UPGRADED v2.0**
Production-ready ensemble learning with 10+ methods for classification and regression.

**Algorithms:**
- **Bagging**: Bagging, Random Forest, Extra Trees
- **Boosting**: AdaBoost, Gradient Boosting, XGBoost*, LightGBM*
- **Meta-Learning**: Voting (soft/hard), Stacking

**Key Features:**
- 10+ ensemble methods supporting both classification and regression
- Hyperparameter tuning with GridSearchCV
- Feature importance analysis
- Out-of-bag (OOB) error estimation
- Model persistence (save/load)
- Performance comparison visualization
- Automatic best model selection

**Technologies:** scikit-learn, XGBoost*, LightGBM*

**Status:** 782 lines, production-ready for both tasks

```bash
cd EnsembleMethods
python ensemble_models.py --data data.csv --target label --task classification --tune --output comparison.png
```

---

### 6. üé® [Clustering](Clustering/)
Unsupervised clustering with automatic cluster number selection.

**Algorithms:**
- K-Means (with elbow method)
- DBSCAN (density-based)
- Hierarchical Clustering

**Key Features:**
- Silhouette score optimization
- PCA visualization
- Elbow method for K-Means
- Dendrogram generation

**Technologies:** scikit-learn, scipy

```bash
cd Clustering
python clustering_analysis.py --data customers.csv --output clusters.png
```

---

### 7. üìâ [Dimensionality Reduction](DimensionalityReduction/)
Reduce feature space while preserving important information.

**Techniques:**
- PCA (Principal Component Analysis)
- t-SNE
- UMAP

**Key Features:**
- Feature space visualization
- Noise reduction
- Data compression
- Pattern discovery

**Technologies:** scikit-learn, umap-learn

```bash
cd DimensionalityReduction
python dimensionality_reduction.py --data highdim.csv --method umap
```

---

### 7. üö® [Anomaly Detection](AnomalyDetection/)
Identify outliers and anomalous patterns in data.

**Algorithms:**
- Isolation Forest
- One-Class SVM
- Local Outlier Factor

**Key Features:**
- Unsupervised detection
- Contamination estimation
- Outlier scoring
- Visualization

**Technologies:** scikit-learn, PyOD

```bash
cd AnomalyDetection
python anomaly_detection.py --data transactions.csv --output anomalies.csv
```

---

### 8. üéÆ [Reinforcement Learning](ReinforcementLearning/)
Learn optimal policies through trial and error.

**Algorithms:**
- Q-Learning
- Deep Q-Networks (DQN)
- Policy Gradients

**Key Features:**
- Environment simulation
- Reward optimization
- Policy learning
- Agent training

**Technologies:** Gym, Stable-Baselines3

```bash
cd ReinforcementLearning
python rl_agent.py --env CartPole-v1 --episodes 1000
```

---

### 9. ü§ñ [AutoML](AutoML/) ‚≠ê **UPGRADED**
Automated machine learning with intelligent model selection and hyperparameter optimization.

**Features:**
- Automatic task type detection (classification/regression)
- Model selection from 6+ algorithms
- Hyperparameter optimization with RandomizedSearchCV
- Time-limited optimization
- Cross-validation and model ranking
- Comprehensive parameter grids

**Technologies:** scikit-learn, RandomizedSearchCV

**Status:** Production-ready with automatic model selection and hyperparameter tuning

```bash
cd AutoML
python automl.py
```

---

### 10. üéØ [Feature Selection](FeatureSelection/) ‚≠ê **UPGRADED**
Advanced feature selection with multiple algorithms and ensemble methods.

**Features:**
- **Filter Methods**: Univariate tests, Mutual Information
- **Wrapper Methods**: RFE, RFECV, Sequential selection
- **Embedded Methods**: L1 regularization, Tree importance, Permutation importance
- **Ensemble Voting**: Combine multiple methods
- **Stability Analysis**: Features selected by majority of methods

**Technologies:** scikit-learn

**Status:** Production-ready with 7+ selection methods and visualization tools

```bash
cd FeatureSelection
python feature_selection.py
```

---

### 11. ‚öñÔ∏è [Imbalanced Learning](ImbalancedLearning/) ‚≠ê **UPGRADED**
Handle imbalanced datasets with advanced resampling techniques.

**Features:**
- **SMOTE**: Synthetic Minority Over-sampling
- **ADASYN**: Adaptive Synthetic Sampling
- **Under-sampling**: Random and Tomek Links
- **Hybrid Methods**: SMOTE + Tomek Links
- **ImbalancedClassifier**: Integrated wrapper
- **Strategy Comparison**: Automatic evaluation

**Technologies:** scikit-learn, custom implementations

**Status:** Production-ready with multiple resampling strategies

```bash
cd ImbalancedLearning
python imbalanced_learning.py
```

---

### 12. üåä [Online Learning](OnlineLearning/)
Incremental learning for streaming data.

**Features:**
- Incremental model updates
- Partial fit methods
- Concept drift detection
- Memory-efficient processing

**Technologies:** scikit-learn, River

```bash
cd OnlineLearning
python online_learning.py
```

---

### 13. üß† [Meta Learning](MetaLearning/)
Learning to learn from multiple tasks.

**Features:**
- Few-shot learning
- Transfer learning
- Model adaptation
- Task similarity analysis

**Technologies:** PyTorch, scikit-learn

```bash
cd MetaLearning
python meta_learning.py
```

---

### 14. üéØ [Multi-Task Learning](MultiTaskLearning/)
Joint learning across related tasks.

**Features:**
- Shared representations
- Task-specific layers
- Multi-objective optimization
- Transfer between tasks

**Technologies:** PyTorch, scikit-learn

```bash
cd MultiTaskLearning
python multi_task_learning.py
```

---

### 15. üîç [Model Interpretability](ModelInterpretability/)
Explain and interpret ML model predictions.

**Techniques:**
- SHAP (SHapley Additive exPlanations)
- LIME (Local Interpretable Model-agnostic Explanations)
- Feature Importance

**Key Features:**
- Model-agnostic explanations
- Individual prediction interpretation
- Feature contribution analysis
- Visualization tools

**Technologies:** SHAP, LIME, scikit-learn

```bash
cd ModelInterpretability
python model_explainer.py --model trained_model.pkl --data test.csv
```

---

## üöÄ Quick Start

### Installation

Each project has its own `requirements.txt`:

```bash
# Install dependencies for specific project
cd RegressionAnalysis
pip install -r requirements.txt
```

### General Dependencies

```bash
pip install numpy pandas scikit-learn matplotlib seaborn statsmodels scipy joblib
```

## üìä Algorithm Comparison

| Algorithm | Type | Use Case | Pros | Cons |
|-----------|------|----------|------|------|
| Linear Regression | Regression | Simple relationships | Fast, interpretable | Assumes linearity |
| Ridge/Lasso | Regression | High-dimensional data | Regularization, feature selection | Needs tuning |
| Logistic Regression | Classification | Binary/multi-class | Simple, probabilistic | Linear boundaries |
| SVM | Classification | Complex boundaries | Kernel trick, robust | Slow on large data |
| Decision Trees | Both | Non-linear patterns | Interpretable | Overfitting risk |
| Random Forest | Both | General purpose | Robust, accurate | Black box |
| K-Means | Clustering | Customer segmentation | Fast, simple | Assumes spherical clusters |
| DBSCAN | Clustering | Arbitrary shapes | No need to specify k | Sensitive to parameters |
| ARIMA | Time Series | Stationary series | Statistical foundation | Needs stationarity |
| PCA/t-SNE | Dimensionality | Visualization | Reduces complexity | May lose information |
| Isolation Forest | Anomaly | Outlier detection | Fast, scalable | Needs tuning |
| Q-Learning | RL | Sequential decisions | Optimal policies | Exploration challenge |
| AutoML | Automation | Quick modeling | Time-saving | Less control |
| SHAP/LIME | Interpretability | Model explanation | Trust, transparency | Computational cost |

## üé® Use Cases by Industry

### üè¢ Finance
- **Regression**: Stock price prediction, risk assessment
- **Classification**: Credit scoring, fraud detection
- **Time Series**: Market forecasting, algorithmic trading

### üè• Healthcare
- **Classification**: Disease diagnosis, patient risk stratification
- **Clustering**: Patient segmentation, treatment grouping
- **Regression**: Cost prediction, treatment outcome

### üõí Retail
- **Clustering**: Customer segmentation, product grouping
- **Time Series**: Demand forecasting, inventory optimization
- **Classification**: Churn prediction, recommendation

### üè≠ Manufacturing
- **Classification**: Quality control, defect detection
- **Regression**: Predictive maintenance, yield prediction
- **Time Series**: Production planning

## üìà Performance Benchmarks

Tested on standard datasets:

| Project | Dataset | Metric | Score | Time (CPU) |
|---------|---------|--------|-------|------------|
| Regression | Boston Housing | R¬≤ | 0.89 | 0.1s |
| Classification | Iris | Accuracy | 0.97 | 0.05s |
| Clustering | Mall Customers | Silhouette | 0.55 | 0.2s |
| Time Series | Air Passengers | RMSE | 18.5 | 0.5s |
| Ensemble | Wine Quality | Accuracy | 0.92 | 1.2s |
| Dimensionality Reduction | MNIST | Variance | 95% | 0.8s |
| Anomaly Detection | Credit Card Fraud | F1 | 0.88 | 0.3s |
| Reinforcement Learning | CartPole | Reward | 195+ | 120s |
| AutoML | Titanic | Accuracy | 0.82 | 300s |
| Model Interpretability | Housing Prices | - | - | 5s |

## üîß Advanced Features

### Cross-Validation

```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X, y, cv=5, scoring='r2')
print(f"CV Score: {scores.mean():.4f} (+/- {scores.std():.4f})")
```

### Hyperparameter Tuning

```python
from sklearn.model_selection import GridSearchCV

param_grid = {'alpha': [0.1, 1, 10], 'max_iter': [1000, 5000]}
grid_search = GridSearchCV(model, param_grid, cv=5)
grid_search.fit(X_train, y_train)
best_model = grid_search.best_estimator_
```

### Pipeline Creation

```python
from sklearn.pipeline import Pipeline

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=10)),
    ('classifier', RandomForestClassifier())
])

pipeline.fit(X_train, y_train)
```

## üìö Learning Path

### Beginner
1. **Start**: Linear Regression, Logistic Regression
2. **Learn**: Model evaluation metrics
3. **Practice**: Simple datasets (Iris, Boston Housing)

### Intermediate
4. **Explore**: Decision Trees, Random Forest
5. **Master**: Cross-validation, hyperparameter tuning
6. **Apply**: Real-world datasets

### Advanced
7. **Deep Dive**: Ensemble methods, stacking
8. **Optimize**: Feature engineering, model selection
9. **Deploy**: Production-ready pipelines

## üéì Educational Value

Each project includes:
- ‚úÖ **Complete Documentation** with theory
- ‚úÖ **Code Examples** with explanations
- ‚úÖ **Use Case Scenarios**
- ‚úÖ **Performance Metrics**
- ‚úÖ **Troubleshooting Guides**
- ‚úÖ **Best Practices**

## üèÜ Key Features Across All Projects

### 1. **Automatic Model Selection**
- Compare multiple algorithms
- Select best performing model
- Cross-validation scores

### 2. **Comprehensive Evaluation**
- Multiple metrics (accuracy, R¬≤, silhouette, etc.)
- Visual comparisons
- Statistical significance tests

### 3. **Production Ready**
- Model persistence (save/load)
- Scalable implementations
- Error handling
- Logging

### 4. **Visualization**
- Performance plots
- Feature importance
- Learning curves
- Prediction visualizations

## üî¨ Theory & References

### Key Concepts

**Bias-Variance Tradeoff**
- **Bias**: Underfitting (model too simple)
- **Variance**: Overfitting (model too complex)
- **Solution**: Regularization, cross-validation

**Regularization**
- **L1 (Lasso)**: Feature selection, sparse solutions
- **L2 (Ridge)**: Coefficient shrinkage, handles multicollinearity

**Ensemble Learning**
- **Bagging**: Reduce variance (Random Forest)
- **Boosting**: Reduce bias (Gradient Boosting)
- **Stacking**: Combine different models

### Recommended Reading

- **Books**:
  - "Hands-On Machine Learning" by Aur√©lien G√©ron
  - "Pattern Recognition and Machine Learning" by Christopher Bishop
  - "The Elements of Statistical Learning" by Hastie, Tibshirani, Friedman

- **Online**:
  - [Scikit-Learn Documentation](https://scikit-learn.org/)
  - [StatQuest YouTube Channel](https://www.youtube.com/c/joshstarmer)
  - [Kaggle Learn](https://www.kaggle.com/learn)

## üêõ Common Issues & Solutions

**Poor Model Performance**
- ‚úÖ Try different algorithms
- ‚úÖ Feature engineering
- ‚úÖ More training data
- ‚úÖ Hyperparameter tuning

**Overfitting**
- ‚úÖ Use regularization (Ridge/Lasso)
- ‚úÖ Reduce model complexity
- ‚úÖ More training data
- ‚úÖ Cross-validation

**Slow Training**
- ‚úÖ Use simpler models first
- ‚úÖ Reduce feature dimensionality (PCA)
- ‚úÖ Sample large datasets
- ‚úÖ Parallel processing

## üìÑ License

MIT License - Free for commercial and research use

---

## üìû Contact

**Author**: BrillConsulting | AI Consultant & Data Scientist

**Email**: clientbrill@gmail.com

**LinkedIn**: [BrillConsulting](https://www.linkedin.com/in/brillconsulting)

---

<p align="center">
  <strong>‚≠ê Star this repository if you find it useful! ‚≠ê</strong>
</p>

<p align="center">
  Made with ‚ù§Ô∏è by BrillConsulting
</p>
