# Model Serving Framework

High-performance model serving infrastructure with FastAPI, Triton Inference Server, vLLM, and Ollama.

## Features

- **FastAPI Integration** - RESTful API for model inference
- **Triton Inference Server** - NVIDIA's production-ready serving platform
- **vLLM Support** - High-throughput LLM inference engine
- **Ollama Integration** - Local LLM serving made simple
- **Multi-Model Management** - Load/unload models dynamically
- **Health Monitoring** - Endpoint health checks and metrics
- **Request Batching** - Automatic batching for throughput
- **Streaming Responses** - Support for streaming outputs
- **Authentication & Security** - API key management

## Supported Backends

| Backend | Use Case | Performance |
|---------|----------|-------------|
| **vLLM** | High-throughput LLM serving | ‚ö° Fastest for batched requests |
| **Triton** | Multi-framework support | üîß Most flexible |
| **Ollama** | Local development | üöÄ Easiest setup |
| **TorchServe** | PyTorch models | üêç Native PyTorch |

## Usage

### Quick Start with vLLM

```python
from model_serving import ModelServer

# Initialize server
server = ModelServer(
    backend="vllm",
    model_name="meta-llama/Llama-2-7b-hf",
    port=8000
)

# Start serving
server.start()
```

### FastAPI Endpoints

```bash
# Health check
curl http://localhost:8000/health

# Generate text
curl -X POST http://localhost:8000/v1/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Hello, my name is",
    "max_tokens": 50,
    "temperature": 0.7
  }'

# Streaming response
curl -X POST http://localhost:8000/v1/generate/stream \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Write a poem", "max_tokens": 100}'
```

### Triton Inference Server

```python
# Deploy model to Triton
from model_serving import TritonDeployer

deployer = TritonDeployer(server_url="localhost:8001")

# Register model
deployer.deploy_model(
    model_name="bert-base",
    model_path="/models/bert",
    platform="pytorch_libtorch"
)

# Inference
result = deployer.infer(
    model_name="bert-base",
    inputs={"input_ids": input_tensor}
)
```

### Ollama Local Serving

```python
from model_serving import OllamaServer

# Start Ollama server
ollama = OllamaServer()
ollama.pull_model("llama2:7b")

# Generate
response = ollama.generate(
    model="llama2:7b",
    prompt="Explain quantum computing"
)
```

## Configuration

```yaml
# config.yaml
server:
  host: "0.0.0.0"
  port: 8000
  workers: 4

backend:
  type: "vllm"
  model: "meta-llama/Llama-2-7b-hf"
  gpu_memory_utilization: 0.9
  max_num_seqs: 256

batching:
  enabled: true
  max_batch_size: 32
  timeout_ms: 100

monitoring:
  prometheus_enabled: true
  metrics_port: 9090
```

## Performance

- **Throughput**: 1000+ req/sec with batching
- **Latency**: <50ms P99 (single request)
- **GPU Utilization**: >85% with optimal batching
- **Concurrent Users**: Supports 100+ concurrent connections

## Demo

```bash
# Start server
python model_serving.py --backend vllm --model llama2-7b

# Run benchmark
python benchmark.py --url http://localhost:8000 --requests 1000
```

## Technologies

- FastAPI 0.104+
- vLLM 0.2+
- Triton Inference Server 2.40+
- Ollama 0.1+
- TorchServe 0.9+
- ONNX Runtime 1.16+
- Prometheus (monitoring)
- Docker & Kubernetes ready
